{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survey Pre-Processing for Cycling in Prague Study\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This Jupyter Notebook demonstrates the pre-processing steps for a survey dataset on cycling in Prague. It's designed to help researchers and data analysts clean, transform, and prepare survey data for further analysis. This script is particularly useful for those working with survey data in urban planning, transportation studies, or social sciences.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- How to import and handle various data types in Python\n",
    "- Techniques for cleaning and standardizing survey responses\n",
    "- Methods for handling geospatial data\n",
    "- Approaches to categorizing and encoding survey responses\n",
    "- Best practices for identifying and flagging potential data quality issues\n",
    "\n",
    "\n",
    "### Libraries Used\n",
    "\n",
    "We'll be using the following Python libraries:\n",
    "\n",
    "- pandas: For data manipulation and analysis\n",
    "- numpy: For numerical operations\n",
    "- datetime: For handling date and time data\n",
    "- json: For JSON data processing (if needed)\n",
    "- geopandas: For geospatial data operations\n",
    "- shapely: For geometric operations\n",
    "- fuzzywuzzy: For string matching and comparison\n",
    "\n",
    "Let's get started by importing these libraries and loading our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the raw data from Qualtrix. \n",
    "\n",
    "pd.read_excel does two things. One it creates a pandas dataframe. This is a data structure like xlsx but for python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "file_path = r\"V:\\Prague_Biking\\Data\\Survey Data\\Cycling in Prague_October 9, 2024_14.08 - Copy.xlsx\"\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to Geo-Areas \n",
    "Since our survey respondents don't know what a Geo-Area is, they have given us their work and home location interms of prague neighbourhoods. Many have misspellings and typos. \n",
    "\n",
    "Rossanne made a list of Geo-Areas corresponding to neighbourhoods. This is a slightly expanded version of that, where we take neighbourhoods which are close but not exactly in a geo-area. \n",
    "\n",
    "The algorithm is nearest geo-area gets a neighbourhood if within a 5km radius. \n",
    "\n",
    "We store the list of neighbourhoods and the corresponding Geo-Areas as a dictionary in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neighborhood dictionary\n",
    "neighborhood_dict = {\n",
    "    'Geo 1': ['Letná', 'Vinohrady', 'Nové Město', 'Josefov', 'Staré Město'],\n",
    "    'Geo 2': ['Břevnov', 'Petřiny', 'Střešovice', 'Holešovice', 'Bořislavka', 'Dejvice'],\n",
    "    'Geo 3': ['Podolí', 'Strahov', 'Smíchov', 'Nusle', 'Pankrác'],\n",
    "    'Geo 4': ['Žižkov', 'Karlín', 'Hloubětín', 'Střížkov', 'Libeň', 'Vysočany', 'Kobylisy'],\n",
    "    'Geo 5': ['Suchdol']\n",
    "}\n",
    "\n",
    "# Flatten the dictionary for easier fuzzy matching\n",
    "all_neighborhoods = [item for sublist in neighborhood_dict.values() for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching neighborhoods to geo-areas\n",
    "\n",
    "In our survey, respondents provided their home and work locations as Prague neighborhoods.\n",
    "However, we want to group these neighborhoods into larger geo-areas for our analysis.\n",
    "\n",
    "We're going to create a function that does two important things:\n",
    "1. It corrects small spelling mistakes or inconsistencies in the neighborhood names.\n",
    "2. It assigns each neighborhood to its corresponding geo-area.\n",
    "\n",
    "This function will use a technique called \"fuzzy matching\". This is helpful because:\n",
    "- It can handle minor spelling errors (e.g., \"Vinhorady\" instead of \"Vinohrady\")\n",
    "- It can deal with missing diacritical marks (e.g., \"Zizkov\" instead of \"Žižkov\")\n",
    "- It can match slight variations in naming (e.g., \"Nove Mesto\" vs \"Nové Město\")\n",
    "\n",
    "By using this approach, we can clean up our data and group it into the geo-areas\n",
    "we defined earlier, making our subsequent analysis more accurate and meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_neighborhood(neighborhood):\n",
    "    if pd.isna(neighborhood):\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # Perform fuzzy matching\n",
    "    match = process.extractOne(neighborhood, all_neighborhoods)\n",
    "    if match[1] >= 80:  # 80% similarity threshold\n",
    "        corrected = match[0]\n",
    "        \n",
    "        # Find the corresponding geo-area\n",
    "        for geo, neighborhoods in neighborhood_dict.items():\n",
    "            if corrected in neighborhoods:\n",
    "                return corrected, geo\n",
    "    \n",
    "    return neighborhood, np.nan  # Return original if no match found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Data Pre-processing Function\n",
    "\n",
    "The `preprocess_data(df)` function is the core of our data cleaning and preparation process. It takes the raw survey data and transforms it into a format ready for analysis. This function performs several key operations:\n",
    "\n",
    "1. Basic data cleaning (removing empty rows, handling missing data)\n",
    "2. Date and time handling\n",
    "3. Text response cleaning and standardization\n",
    "4. Neighborhood correction and geo-area assignment\n",
    "5. Identification of potential print surveys\n",
    "6. Consistency checks for conditional questions\n",
    "7. Encoding of multiple-choice questions\n",
    "8. Age categorization\n",
    "9. Survey duration calculation\n",
    "10. Data quality flagging\n",
    "11. Language identification\n",
    "12. Likert scale processing\n",
    "\n",
    "This comprehensive process ensures our data is clean, consistent, and properly formatted for in-depth analysis of cycling patterns in Prague."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Debugging: Print column names and data types\n",
    "    print(\"Column names:\", df.columns.tolist())\n",
    "    print(\"\\nData types:\\n\", df.dtypes)\n",
    "    \n",
    "    # Step 1: Basic Data Cleaning\n",
    "    df = df.replace({'': np.nan, ' ': np.nan})  # Replace empty strings and spaces with NaN\n",
    "    df = df.dropna(how='all')  # Drop rows that are all NaN\n",
    "    world_shapefile_path = r\"V:\\Prague_Biking\\Data\\Maps\\ne_110m_admin_0_countries.shp\"\n",
    "    df = process_ip_locations(df, world_shapefile_path)\n",
    "    # Step 2: Handle dates\n",
    "    date_columns = ['StartDate', 'EndDate', 'RecordedDate']\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "    # Step 3: Clean text responses\n",
    "    text_columns = ['Q7', 'Q9', 'Q11', 'Q14_9_TEXT', 'Q17_9_TEXT', 'Q21_10_TEXT', 'Q22_10_TEXT', 'Q28_7_TEXT', 'Q29_6_TEXT', 'Q33_6_TEXT', 'Q34']\n",
    "    for col in text_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].str.strip().str.lower()\n",
    "    \n",
    "    if 'Q7' in df.columns:\n",
    "        df['Q7_corrected'], df['Q7a'] = zip(*df['Q7'].apply(correct_neighborhood))\n",
    "    \n",
    "    if 'Q11' in df.columns:\n",
    "        df['Q11_corrected'], df['Q8a'] = zip(*df['Q11'].apply(correct_neighborhood))\n",
    "    # Step 4: Identify potential print surveys\n",
    "    df['potential_print_survey'] = df['RecordedDate'].dt.date == datetime(2024, 10, 8).date()\n",
    "\n",
    "    # Step 5: Handle conditional questions\n",
    "    conditional_pairs = [\n",
    "        ('Q6', 'Q7'),  # If lives in Prague, neighborhood\n",
    "        ('Q8', 'Q9'),  # If works in Prague, work neighborhood\n",
    "        ('Q10', 'Q11'),  # If studies in Prague, study neighborhood\n",
    "    ]\n",
    "    for condition_q, dependent_q in conditional_pairs:\n",
    "        if condition_q in df.columns and dependent_q in df.columns:\n",
    "            mask = (df[condition_q] == 'No') & df[dependent_q].notna()\n",
    "            df.loc[mask, 'data_inconsistency'] = True\n",
    "\n",
    "    # Step 6: Encode multiple-choice questions\n",
    "    multiple_choice_columns = ['Q4', 'Q14', 'Q17', 'Q20', 'Q21', 'Q22', 'Q28', 'Q29', 'Q30', 'Q31']\n",
    "    for col in multiple_choice_columns:\n",
    "        if col in df.columns:\n",
    "            dummies = df[col].str.get_dummies(sep=',')\n",
    "            dummies.columns = [f\"{col}_{c}\" for c in dummies.columns]\n",
    "            df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "    # Step 7: Age validation\n",
    "    if 'Q3' in df.columns:\n",
    "        df['Q3'] = pd.Categorical(df['Q3'], categories=['Under 18', '18 - 24', '25 - 34', '35 - 44', '45 - 54', '55 - 64', '65  or older'], ordered=True)\n",
    "\n",
    "    # Step 8: Calculate survey duration\n",
    "    if 'StartDate' in df.columns and 'EndDate' in df.columns:\n",
    "        df['calculated_duration'] = (df['EndDate'] - df['StartDate']).dt.total_seconds()\n",
    "\n",
    "    # Step 9: Flag potential data quality issues\n",
    "    if 'Progress' in df.columns:\n",
    "        # Convert 'Progress' to numeric, replacing any non-numeric values with NaN\n",
    "        df['Progress'] = pd.to_numeric(df['Progress'], errors='coerce')\n",
    "        \n",
    "        df['potential_issue'] = np.where(\n",
    "            (df['Progress'] < 100) |  # Incomplete surveys\n",
    "            (df.get('calculated_duration', pd.Series()) < 60) |  # Very quick responses\n",
    "            (df.get('data_inconsistency', pd.Series()) == True),  # Inconsistent conditional responses\n",
    "            True, False\n",
    "        )\n",
    "    else:\n",
    "        print(\"Warning: 'Progress' column not found in the dataset.\")\n",
    "\n",
    "    # Step 10: Handle language differences\n",
    "    if 'UserLanguage' in df.columns:\n",
    "        df['survey_language'] = np.where(df['UserLanguage'] == 'CS', 'CS', 'EN')\n",
    "    else:\n",
    "        print(\"Warning: 'UserLanguage' column not found in the dataset.\")\n",
    "\n",
    "    # Step 11: Process Likert scale questions\n",
    "    likert_questions = ['Q24', 'Q29', 'Q26', 'Q27']\n",
    "    for q in likert_questions:\n",
    "        if q in df.columns:\n",
    "            df[q] = pd.Categorical(df[q], categories=['Strongly agree', 'Agree', 'Disagree', 'Strongly disagree'], ordered=True)\n",
    "\n",
    "    # Step 12: Process slider question\n",
    "    if 'Q24' in df.columns:\n",
    "        df['Q24'] = pd.to_numeric(df['Q24'], errors='coerce')\n",
    "\n",
    "    # Debugging: Print column names and data types after processing\n",
    "    print(\"\\nColumn names after processing:\", df.columns.tolist())\n",
    "    print(\"\\nData types after processing:\\n\", df.dtypes)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperating Online and Offline Surveys : \n",
    "\n",
    "Approach : All Offline surveys were entered into Qualtrix. This was done after the end of the field trip, between 8th and 9th October. Qualtrix records the IP address, location, and time of entry. \n",
    "\n",
    "By selecting all IP addresses which entered more than 1 survey, in the netherlands we can isolate all the surveys we entered into the system manually. \n",
    "\n",
    "Surveys filled in Prague have to be online since we did not manually input survey forms in prague. \n",
    "\n",
    "\n",
    "We categorize the survey data into 4 buckets, This is because of two reasons :\n",
    "1. Online surveys have conditional logic, which means questions were shown based on previous answers. Ex the question on \"What encourages you to bike?\" was shown only to people who had selected Yes to Owning a bike or If they used bike sharing.  Offline surveys on the other hand did not have this rich conditional logic as they were printed on paper. \n",
    "\n",
    "2. While entering offline surveys back into qualtrics, we initially left the conditional logic on. This meant that certain answers were ignored from the print surveys because Qualtrix did not display the question. After survey 508, it was decided to disable conditional logic on qualtrix to fully represent offline surveys. This is the reason for the 508 and rest differentiation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurvey_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m508\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m508\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m---> 50\u001b[0m processed_df \u001b[38;5;241m=\u001b[39m preprocess_data(\u001b[43mdf\u001b[49m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Split and save the data into four categories\u001b[39;00m\n\u001b[0;32m     53\u001b[0m categories \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOnline_508\u001b[39m\u001b[38;5;124m'\u001b[39m: (processed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurvey_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOnline\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m&\u001b[39m (processed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurvey_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m508\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOffline_508\u001b[39m\u001b[38;5;124m'\u001b[39m: (processed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurvey_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOffline\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m&\u001b[39m (processed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurvey_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m508\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOnline_Rest\u001b[39m\u001b[38;5;124m'\u001b[39m: (processed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurvey_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOnline\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m&\u001b[39m (processed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurvey_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRest\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOffline_Rest\u001b[39m\u001b[38;5;124m'\u001b[39m: (processed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurvey_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOffline\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m&\u001b[39m (processed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurvey_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     58\u001b[0m }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def process_ip_locations(df, world_shapefile_path):\n",
    "    # Check if required columns exist\n",
    "    required_columns = ['IPAddress', 'LocationLatitude', 'LocationLongitude']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        print(\"Error: One or more required columns not found in the dataset.\")\n",
    "        return df\n",
    "    \n",
    "    # Read world shapefile\n",
    "    world = gpd.read_file(world_shapefile_path)\n",
    "    \n",
    "    # Try to identify the country column\n",
    "    country_column = next((col for col in ['COUNTRY', 'NAME', 'ADMIN', 'SOVEREIGNT'] if col in world.columns), None)\n",
    "    if country_column is None:\n",
    "        print(\"Error: Could not identify a suitable country column in the world shapefile.\")\n",
    "        return df\n",
    "    \n",
    "    print(f\"Using '{country_column}' as the country column.\")\n",
    "    \n",
    "    # Convert longitude and latitude to numeric, replacing any non-numeric values with NaN\n",
    "    df['LocationLongitude'] = pd.to_numeric(df['LocationLongitude'], errors='coerce')\n",
    "    df['LocationLatitude'] = pd.to_numeric(df['LocationLatitude'], errors='coerce')\n",
    "    \n",
    "    # Create geometry column, filtering out rows with NaN coordinates\n",
    "    valid_coords = df[['LocationLongitude', 'LocationLatitude']].notna().all(axis=1)\n",
    "    geometry = [Point(xy) for xy in zip(df.loc[valid_coords, 'LocationLongitude'], df.loc[valid_coords, 'LocationLatitude'])]\n",
    "    gdf = gpd.GeoDataFrame(df[valid_coords], geometry=geometry, crs=\"EPSG:4326\")\n",
    "    \n",
    "    # Perform spatial join\n",
    "    gdf_with_country = gpd.sjoin(gdf, world[['geometry', country_column]], how=\"left\", predicate=\"within\")\n",
    "    \n",
    "    # Categorize as Offline (Netherlands) or Online (rest)\n",
    "    def categorize_survey_type(country, ip):\n",
    "        if pd.isna(country) or pd.isna(ip) or ip.lower() in ['anon', 'anonymous', 'na', 'n/a', '']:\n",
    "            return \"Unknown\"\n",
    "        elif country == \"Netherlands\":\n",
    "            return \"Offline\"\n",
    "        else:\n",
    "            return \"Online\"\n",
    "    \n",
    "    gdf_with_country['survey_type'] = gdf_with_country.apply(lambda row: categorize_survey_type(row[country_column], row['IPAddress']), axis=1)\n",
    "    \n",
    "    # Merge the results back to the original dataframe\n",
    "    df = df.merge(gdf_with_country[['survey_type']], left_index=True, right_index=True, how='left')\n",
    "    df['survey_type'] = df['survey_type'].fillna('Unknown')\n",
    "    \n",
    "    # Categorize based on survey number\n",
    "    df['survey_version'] = np.where(df.index <= 508, '508', 'Rest')\n",
    "    \n",
    "    return df\n",
    "processed_df = preprocess_data(df)\n",
    "\n",
    "# Split and save the data into four categories\n",
    "categories = {\n",
    "    'Online_508': (processed_df['survey_type'] == 'Online') & (processed_df['survey_version'] == '508'),\n",
    "    'Offline_508': (processed_df['survey_type'] == 'Offline') & (processed_df['survey_version'] == '508'),\n",
    "    'Online_Rest': (processed_df['survey_type'] == 'Online') & (processed_df['survey_version'] == 'Rest'),\n",
    "    'Offline_Rest': (processed_df['survey_type'] == 'Offline') & (processed_df['survey_version'] == 'Rest')\n",
    "}\n",
    "\n",
    "for category, mask in categories.items():\n",
    "    output_path = f\"V:\\\\Prague_Biking\\\\Data\\\\Survey Data\\\\Processed_Cycling_in_Prague_Survey_{category}.xlsx\"\n",
    "    processed_df[mask].to_excel(output_path, index=False)\n",
    "    print(f\"Processed data for {category} saved to {output_path}\")\n",
    "\n",
    "# Save the full processed data\n",
    "full_output_path = r\"V:\\Prague_Biking\\Data\\Survey Data\\Processed_Cycling_in_Prague_Survey_Full.xlsx\"\n",
    "processed_df.to_excel(full_output_path, index=False)\n",
    "print(f\"Full processed data saved to {full_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Biking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
